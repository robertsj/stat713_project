---
title: "Dynamic Mode Decomposition as a Linear Model for Spatio-Temporal Systems"
author: "Jeremy Roberts"
date: "December 1, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

This notebook includes the basic analysis shown in the final report.  The report includes the necessary theory (equations, etc.), while this notebook puts the code into action.

# Load Data

First, load the maximum temperature data for July of 1993:

```{r tidy=TRUE,message=FALSE,warning=FALSE}
library("dplyr")
library("STRbook")
data("NOAA_df_1990", package = "STRbook")
df <- filter(NOAA_df_1990, year==1993, month==7, proc=="Tmax")
```

Now, we need to clean up the data to include only those stations that have measurements for each day of the month.  This greatly simplifies the DMD analysis, which is based on a dense [space, time] snapshot matrix.

There are 31 days in July, so search for all those station id's that have 31 measurements and filter the data to include only those.

```{r tidy=TRUE,message=FALSE,warning=FALSE}
keep <- c()
for (station in unique(df$id)){
  if (length(filter(df,id==station)$julian)==31){
    keep <- append(keep, station)
  }
}
df <- filter(df, id%in%keep)
```

The time coordinate is a cardinal month index.  Here, add a new column for time that translates the time to start at zero:

```{r}
df <- mutate(df, t=df$julian-min(df$julian))
```

Finally, to clean things up a bit, keep only `t`,  `id` (the station number), `z` (the maximum temperature), `lat` (latitude), and `lon` (longitude):
```{r}
df <- subset(df, select=c(t, id, z, lat, lon))
head(df)
```

# Construct Snapshot Matrix

DMD is applied to a matrix $\mathbf{Z} \in \mathbb{R}^{m,n}$, or

```{r tidy=TRUE,message=FALSE,warning=FALSE}
num_locs <- length(unique(df$id))
num_time <- length(df$id)/num_locs
Z <- t(matrix(as.numeric(df$z), num_time, num_locs, byrow = FALSE))
```

One way to view this data is by treating the elements of $\mathbf{Z}$ as pixels of an image (or a "heatmap").  Here, the temperature is shown on a 2-D grid of location (an arbirary index for each weather station) and day (in July):

```{r tidy=TRUE,message=FALSE,warning=FALSE}
library(ggplot2)
heat <- expand.grid(time=1:num_time, location=1:num_locs)
heat$z <- df$z
ggplot(heat, aes(location, time, fill=z)) + geom_tile() + scale_fill_viridis_c(option="inferno")
ggsave("snapshots.pdf")
```

# A DMD-Generated Basis for Statistical Linear Modeling: The Basic Approach

Given $\mathbf{Z}$, $\mathbf{Z}_- = \mathbf{Z}_{:,1:n-1}$, and $\mathbf{Z}_+ = \mathbf{Z}_{:, 2:n}$, the DMD algorithm employs the SVD to compute a rank-$r$ operator $\mathbf{A}_r$ that approximately satisfies $\mathbf{Z}_+ = \mathbf{A}_r\mathbf{Z}_-$.  The eigenvectors $\boldsymbol{\phi}_i$ of $\mathbf{A}_r$ are the DMD "modes", while  the corresponding eigenvalues $\lambda_i$ yield the DMD frequencies $\omega_i = \log \lambda_i / \Delta_t$ for observations separated by a fixed $\Delta_t$ in time.

To support modeling of $\mathbf{z}(t)$ given the observed data, construction and use of a linear model based on `lm` requires a few steps:

  1. Generate the DMD modes $\boldsymbol{\phi}_i$ and frequences $\omega_i$ given known observations and times.
  2. Construct spatio-temporal DMD basis vectors $\boldsymbol{\psi}_i$ using the same observations and times.
  3. Use `lm` to fit the model $\mathbf{z} \sim \mathcal{N}(\boldsymbol{\Psi}\boldsymbol{\beta}, \sigma^2\mathbf{I})$, where the DMD basis vectors are the covariates.  This requires that a dataframe be generated with columns for each of those basis vectors. 
  4. Predictions should be made with `lm` for possibly different times than were used in the original fitting.  In other words, the basis vectors may need to be reconstructed at new times with which a new dataframe is constructed.
  
To support these steps, several functions are implemented in `dmd.R`:

  - `compute_svd(Z, r)` computes the SVD of a matrix $\mathbf{Z}$ with a specified rank $r$.  If $r > 0$, the leading $r$ singular vectors and values are retained.  If $r = 0$, the optimal rank is selected (based on the criterion of Davish and Donoho).  If $r < 0$, then the maximum possible rank is used (i.e., $\text{max}(n, m)$ for $\mathbf{Z}\in \mathbb{R}^{m\times n}$).  A `list` with entries `u`, `v`, and `d` is returned, just like the R-function `svd`.
  - `compute_dmd(Z, rank, delta.t)` computes the DMD modes and frequencies given $\mathbf{Z}$, a desired rank, and a fixed $\Delta_t$.  The rank is the same used for the SVD.  However, because complex-conjugate pairs are reduced to their real components, the final rank of the DMD modes may be reduced.  A list with entries `Phi`, `omega`, `rank`, and `delta.t` is returned, where this `rank` is the final one.
  - `compute_dmd_basis(dmd, t)` computes the DMD basis vectors given the modes `dmd$Phi`, frequencies `dmd$omega`, and times `t`.  The matrix `\boldsymbol{\Psi} \in \mathbb{R}^{mn, r}$ is returned, where $r$ is the number of DMD modes.
  - `df_add_dmd(df, dmd)` inserts a DMD basis computed using `dmd` into a dataframe `df`.  The dataframe must have observations organized in a regular space-time grid (with time as the inner variable).  Times may be arbitrary (relative to a starting point of 0), but spatial points must be consistent with those of the DMD modes.  Columns added are `Psi.1`, `Psi.2`, ..., `Psi.R`.
  - `lm_with_dmd(df, rank)` produces a linear model using the equivalent of `z ~ Psi.1 + Psi.2 + ... + Psi.R - 1`, i.e., the intercept is removed because `Psi.1` captures the "average" dynamics.  Returns a list with `model` and `dmd` attributes.
  - `predict_with_dmd(df, dmdmodel)` is given (a possibly new) dataframe and list `dmdmodel` from `lm_with_dmd`.  Returns the result of `predict` with 95% prediction intervals.
  ###
  
# Inspecting DMD Modes

With the helper functions in place, it is useful to examine what goes into a DMD basis.  Here, the full SVD and the optimally truncated SVD are computed to show where the "optimal" criterion sits for this data:

```{r,  fig.height=3, fig.width=5}
source("dmd.R")
library(latex2exp)
pdf("singular_values.pdf",width=5,height=4)
svd_f <- compute_svd(Z, rank=-1)
svd_t <- compute_svd(Z, rank=0)
plot(svd_f$d, log="y", col="black",  xlab="", ylab="")
title(xlab=TeX('$i$'), ylab=TeX('$\\sigma_i$'))
r <- length(svd_t$d)
text(x=12, y=1.1*svd_t$d[r], labels = sprintf("optimal rank is %i", r), pos = 3., adj=c(0,0), col="red")
points(svd_t$d, col="red", pch=18)
dev.off()
```

The optimal rank is `r r`.

With this rank, compute the DMD modes and amplitudes.  Here, just four modes are kept due to elimination of complex conjugates.  These are normalized to 1 and shown as a function of station latitude.  The first mode captures the anticipated dependence on latitude, i.e., with increasing latitude, the temperature should drop off.  The specific "sign" of $\boldsymbol{\phi}_1$ is coincidentally in the right direction, but its the negative correlation that is important.  The other modes are far harder to interpret.

```{r}
dmd <- compute_dmd(Z, rank=0)
tmp <- sort(filter(df, t==1)$lat, index.return=TRUE)
ix <- tmp$ix
lats <- tmp$x
pdf("dmd_modes.pdf")
par(mfrow=c(2, 2))
for (r in 1:4)
{
  y <-  Re(dmd$Phi[ix,r])
  y <- y / sqrt(sum(y^2))
  plot(lats, y, type="o", xlab="", ylab="")
  title(xlab="latitude (deg.)", ylab=TeX(sprintf("$\\phi_{%i}$", r)))
}
dev.off()
```
  
# Reconstructing The Original Data

Given the entire set of data $\mathbf{Z}$, how well can the DMD modes be used to recapture that data?  Below, three DMD models are used to predict the temperature at each location and day.  These models use 3, 4, and 16 (the maximum possible given elimination of complex-conjugate pairs).  The errors are shown to the right.  As an alternative, several polynomial models are also constructed.  The title of each image summarizes the model, the (estimated) adjusted $R^2$, and the AIC value.

```{r tidy=TRUE,message=FALSE,warning=FALSE, fig.height=10, fig.width=8.25}
require(gridExtra)

models = list()
# Make DMD models and predictions
for (r in c(5, 7, -1)){
  tmp <- lm_with_dmd(df, rank=r, keep_pairs = 2)
  tmp$pred <- predict_with_dmd(df, tmp$model, tmp$dmd)
  tmp$desc <- sprintf("DMD, rank=%i", tmp$dmd$rank)
  models[[length(models)+1]] <- tmp
}
for (i in 1:3)
{
  desc <- "z ~ (t+lat+lon)"
  if (i > 1){desc <- sprintf("z ~ (t+lat+lon)^%i",i)}
  tmp  <- list(model=lm(formula(desc), data=df))
  tmp$pred <- predict(tmp$model, interval="prediction")
  tmp$desc <- desc
  models[[length(models)+1]] <- tmp
}

# Given data, the model, and the prediction, make a plot of the prediction,
# make a plot of the prediction error, and compute a few key quantities.
p = list()
for (i in 1:length(models)){
  m <- models[[i]]

  heat <- expand.grid(time=1:num_time, location=1:num_locs)
  heat$z <-m$pred[, 1]
  p[[length(p) + 1]] <- ggplot(heat, aes(location, time, fill=z)) +
                        geom_tile() + theme_void() + labs(title=m$desc) +
                        scale_fill_continuous(limits=c(65, 105), type = "viridis", option="inferno")
  
  aic <- AIC(m$model)
  desc <- sprintf("AIC = %.2f", aic)
  heat$error <- m$pred[, 1] - df$z
  print(sprintf("%.2f %.2f", max(heat$error), min(heat$error)))

  p[[length(p) + 1]] <- ggplot(heat, aes(location, time, fill=error)) + 
                        geom_tile() + scale_fill_distiller(palette = "Spectral", limits=c(-25, 25)) + theme_void() +
                        labs(title=desc) 
  
  p[[length(p) + 1]]  <- ggplot(filter(cbind(df, m$pred), id==14943), aes(t, z)) +
       geom_point() +
       geom_line(aes(y = fit), color="blue") +
       geom_line(aes(y = upr), color = "red", linetype = "dashed") +
       geom_line(aes(y = lwr), color = "red", linetype = "dashed") 
}

q <- grid.arrange(grobs=p, ncol=3)
ggsave("model_comparison.png", q, dpi = 600, width = 12, height = 12, units = "in")
ggsave("model_comparison.pdf", q, dpi = 600, width = 12, height = 12, units = "in")

```

From these results, it appears the DMD-based models lead to lower AIC values than the simple polynomial models. 
Moreover, the DMD model based on the full SVD yields the lowest AIC value despite having the largest number of parameters to estimate.
Hence, let's investigate the impact of DMD rank on AIC.  As noted, the "optimal" rank is based on an optimally truncated SVD that is itself based on statistical considerations.  Here, AIC is computed for DMD as a function of rank.  Note, complex-conjugate pairs are not reduced to one basis vector this time, though `lm` does in all cases eliminate the singularities.

```{r}
aic <- rep(0, 31)
aic2 <- rep(0, 31)
ranks <- rep(0, 31)
#pdf("dmd_aic.pdf",width=5,height=3.5)
for (r in 1:31){
  m <- lm_with_dmd(df, rank=r, keep_pairs=0)
  aic[r] <- AIC(m$model)
  ranks[r] <- r-sum(is.na(m$model$coefficients))
  m <- lm_with_dmd(df, rank=r, keep_pairs=2)
  aic2[r] <- AIC(m$model)
}
plot(1:31, aic, type="b", pch=19, col="black", xlab="DMD rank", ylab="AIC", ylim=c(min(aic,aic2), max(aic, aic2)))
lines(1:31, aic2, pch=18, col="red", type="b", lty=2)
legend(1, 30000, legend=c("eliminate", "keep"),  col=c("black", "red"), lty=1:2, cex=0.8, pch=c(19,18))
#dev.off()
```

The results indicate that the AIC generally decreases by a small amount as the number of modes include increases, with notable deviations between 20 and 26.  Recall that the optimal SVD was 7, for which 4 DMD modes are retained (3 complex pairs reduced to 3 modes).

Because AIC is itself a statistic, a more careful comparison would estimate confidence intervals for AIC.  However, because the DMD basis requires multiple steps to construct and requires a constant space-time grid, handling of resampled data would be a major challenge.

```{r tidy=TRUE,message=FALSE,warning=FALSE}
for (i in 1:6)
{
  print(summary(models[[i]]$model))
}
```


# Making Predictions

Here, essentially the same analysis is performed, but models are fitted using only the first 28 days of data.


```{r tidy=TRUE,message=FALSE,warning=FALSE, fig.height=10, fig.width=8.25}
require(gridExtra)

df_partial <- filter(df, t < 28)

models2 = list()
# Make DMD models and predictions
for (r in c(5, 7, -1)){
  tmp <- lm_with_dmd(df_partial, rank=r,keep_pairs = 2)
  tmp$pred <- predict_with_dmd(df, tmp$model, tmp$dmd)
  tmp$desc <- sprintf("DMD, rank=%i", tmp$dmd$rank)
  models2[[length(models2)+1]] <- tmp
}
for (i in 1:3)
{
  desc <- "z ~ (t+lat+lon)"
  if (i > 1){desc <- sprintf("z ~ (t+lat+lon)^%i",i)}
  tmp  <- list(model=lm(formula(desc), data=df_partial))
  tmp$pred <- predict(tmp$model, df, interval="prediction")
  tmp$desc <- desc
  models2[[length(models2)+1]] <- tmp
}

# Given data, the model, and the prediction, make a plot of the prediction,
# make a plot of the prediction error, and compute a few key quantities.
p = list()
for (i in 1:length(models2)){
  m <- models2[[i]]

  heat <- expand.grid(time=1:num_time, location=1:num_locs)
  heat$z <-m$pred[, 1]
  p[[length(p) + 1]] <- ggplot(heat, aes(location, time, fill=z)) +
                        geom_tile() + theme_void() + labs(title=m$desc) +
                        scale_fill_continuous(limits=c(65, 105), type = "viridis", option="inferno")
  
  aic <- AIC(m$model)
  desc <- sprintf("AIC = %.2f", aic)
  heat$error <- m$pred[, 1] - df$z
  p[[length(p) + 1]] <- ggplot(heat, aes(location, time, fill=error)) + 
                        geom_tile() + scale_fill_distiller(palette = "Spectral", limits=c(-25, 25)) + theme_void() +
                        labs(title=desc) 
  print(sprintf("%.2f %.2f", max(heat$error), min(heat$error)))
  p[[length(p) + 1]]  <- ggplot(filter(cbind(df, m$pred), id==14943), aes(t, z)) +
       geom_point() +
       geom_line(aes(y = fit), color="blue") +
       geom_line(aes(y = upr), color = "red", linetype = "dashed") +
       geom_line(aes(y = lwr), color = "red", linetype = "dashed") 
}

q <- grid.arrange(grobs=p, ncol=3)#, theme(plot.margin=margin(250,10,10,10)))
ggsave("prediction_comparison.png", q, dpi = 600, width = 12, height = 12, units = "in")
ggsave("prediction_comparison.pdf", q, dpi = 600, width = 12, height = 12, units = "in")
```

# More on Model Selection



